# trigger:
#   branches:
#     include:
#       - main
#       - feature/*

# pool:
#   vmImage: ubuntu-latest

# variables:
#   PROJECT_ID: 'fluted-factor-438905-d2'
#   REGION: 'asia-south1'
#   REPO_NAME: 'todo-repo'
#   SERVICE_ACCOUNT: 'todo-deployer@fluted-factor-438905-d2.iam.gserviceaccount.com'
#   CD_PIPELINE_NAME: 'azure-pipelines-cd.yml'  # Name of the CD pipeline file

# stages:
# # ---------- Stage 1: Build and Push ----------
# - stage: Build
#   displayName: Build and Push Docker Images
#   jobs:
#   - job: BuildAndPush
#     displayName: Build & Push to Google Artifact Registry
#     steps:
#     - checkout: self

#     #  Download Secure File (GCP Service Account JSON)
#     - task: DownloadSecureFile@1
#       name: GCloudKey
#       inputs:
#         secureFile: 'key.json'   # file uploaded to Secure Files

#     # Authenticate to Google Cloud
#     - task: Bash@3
#       displayName: 'Authenticate to Google Artifact Registry'
#       inputs:
#         targetType: 'inline'
#         script: |
#           echo "Authenticating with Google Cloud using secure key..."
#           gcloud auth activate-service-account $SERVICE_ACCOUNT --key-file "$(GCloudKey.secureFilePath)"
#           gcloud config set project $PROJECT_ID
#           gcloud auth configure-docker $REGION-docker.pkg.dev --quiet
#       env:
#         PROJECT_ID: $(PROJECT_ID)
#         REGION: $(REGION)
#         SERVICE_ACCOUNT: $(SERVICE_ACCOUNT)

#     # Build Docker Images
#     - task: Bash@3
#       displayName: 'Build Docker Images'
#       inputs:
#         targetType: 'inline'
#         script: |
#           echo "Building Docker images..."
#           docker build -t frontend:$(Build.BuildId) ./frontend
#           docker build -t backend:$(Build.BuildId) ./backend

#     # Push Docker Images to Google Artifact Registry
#     - task: Bash@3
#       displayName: 'Push to Google Artifact Registry'
#       inputs:
#         targetType: 'inline'
#         script: |
#           echo "Tagging and pushing images to Artifact Registry..."
#           docker tag frontend:$(Build.BuildId) $REGION-docker.pkg.dev/$PROJECT_ID/$REPO_NAME/frontend:$(Build.BuildId)
#           docker tag backend:$(Build.BuildId) $REGION-docker.pkg.dev/$PROJECT_ID/$REPO_NAME/backend:$(Build.BuildId)

#           docker push $REGION-docker.pkg.dev/$PROJECT_ID/$REPO_NAME/frontend:$(Build.BuildId)
#           docker push $REGION-docker.pkg.dev/$PROJECT_ID/$REPO_NAME/backend:$(Build.BuildId)

#     # Publish Kubernetes manifests for CD stage
#     - publish: k8s
#       artifact: k8s-manifests
#       displayName: 'Publish Kubernetes manifests as artifacts'

trigger:
  branches:
    include:
      - main
      - feature/*

pool:
  vmImage: ubuntu-latest

variables:
  PROJECT_ID: 'fluted-factor-438905-d2'
  REGION: 'asia-south1'
  CLUSTER_ZONE: 'asia-south1-a'
  REPO_NAME: 'todo-repo'
  CLUSTER_NAME: 'todo-cluster'
  SERVICE_ACCOUNT: 'todo-deployer@fluted-factor-438905-d2.iam.gserviceaccount.com'

stages:

# STAGE 1 — BUILD & PUSH DOCKER IMAGES

- stage: Build
  displayName: Build and Push Docker Images
  jobs:

  - job: BuildAndPush
    steps:
    - checkout: self

    #  Download Secure File (GCP SA Key)
    - task: DownloadSecureFile@1
      name: GCloudKey
      inputs:
        secureFile: 'key.json'

    #  Authenticate with Google Cloud
    - task: Bash@3
      displayName: 'GCloud Authenticate'
      inputs:
        targetType: inline
        script: |
          echo "Activating Service Account..."
          gcloud auth activate-service-account $SERVICE_ACCOUNT --key-file "$(GCloudKey.secureFilePath)"
          gcloud config set project $PROJECT_ID
          gcloud auth configure-docker $REGION-docker.pkg.dev --quiet


    # INSTALL TERRAFORM
    - task: TerraformInstaller@1
      displayName: "Install Terraform"
      inputs:
        terraformVersion: '1.6.0'

    # TERRAFORM APPLY
    # - task: Bash@3
    #   displayName: "Terraform Apply"
    #   inputs:
    #     targetType: inline
    #     script: |
    #       cd $(Build.SourcesDirectory)/terraform

    #       terraform init

    #       BASE64_KEY=$(cat "$(GCloudKey.secureFilePath)" | base64 | tr -d '\n')

    #       cat <<EOF > terraform.auto.tfvars
    #       project_id      = "${PROJECT_ID}"
    #       region          = "${REGION}"
    #       zone            = "${CLUSTER_ZONE}"
    #       gcp_credentials = "${BASE64_KEY}"
    #       EOF

    #       echo "Generated terraform.auto.tfvars:"
    #       cat terraform.auto.tfvars

    #       terraform apply -auto-approve
        # TERRAFORM APPLY (with auto-import if resources already exist)
    - task: Bash@3
      displayName: "Terraform Init / Import (if exists) / Apply"
      inputs:
        targetType: inline
        script: |
          set -euo pipefail

          cd $(Build.SourcesDirectory)/terraform

          echo "Initializing Terraform..."
          terraform init -input=false

          echo "Preparing terraform.auto.tfvars with base64 encoded key..."
          # base64 on Linux: -w 0 to avoid newlines; tr -d '\n' is fallback
          BASE64_KEY=$(base64 -w 0 "$(GCloudKey.secureFilePath)" 2>/dev/null || cat "$(GCloudKey.secureFilePath)" | base64 | tr -d '\n')

          cat <<EOF > terraform.auto.tfvars
          project_id      = "${PROJECT_ID}"
          region          = "${REGION}"
          zone            = "${CLUSTER_ZONE}"
          gcp_credentials = "${BASE64_KEY}"
          EOF

          echo "terraform.auto.tfvars written:"
          sed -n '1,120p' terraform.auto.tfvars | sed 's/^\(.*gcp_credentials = \)".*"$/\1"<REDACTED>"/'

          # Ensure gcloud is authenticated (should already be from earlier step in pipeline)
          echo "Verifying gcloud auth..."
          gcloud auth activate-service-account $SERVICE_ACCOUNT --key-file "$(GCloudKey.secureFilePath)" || true
          gcloud config set project $PROJECT_ID

          # 1) Import existing GKE cluster (if it exists)
          echo "Checking for existing GKE cluster: ${CLUSTER_NAME} in zone ${CLUSTER_ZONE}..."
          if gcloud container clusters describe "${CLUSTER_NAME}" --zone "${CLUSTER_ZONE}" --project "${PROJECT_ID}" >/dev/null 2>&1; then
            echo "Cluster exists. Attempting terraform import for google_container_cluster.todo_cluster..."
            # Build the import id in the required format: projects/PROJECT/locations/ZONE/clusters/CLUSTER
            CLUSTER_IMPORT_ID="projects/${PROJECT_ID}/locations/${CLUSTER_ZONE}/clusters/${CLUSTER_NAME}"
            # safe import: if already imported terraform import will exit non-zero, so capture and continue
            if terraform state list | grep -q "^google_container_cluster\.todo_cluster$"; then
              echo "google_container_cluster.todo_cluster already in state — skipping import."
            else
              echo "Importing cluster resource into terraform state..."
              terraform import -input=false google_container_cluster.todo_cluster "${CLUSTER_IMPORT_ID}" || {
                echo "terraform import for cluster returned non-zero. Continuing; if this is 'already imported', it's ok."
              }
            fi
          else
            echo "Cluster not found in GCP. Terraform will create it."
          fi

          # 2) Import existing Artifact Registry repo (if it exists)
          echo "Checking for existing Artifact Registry repo: ${REPO_NAME} in region ${REGION}..."
          if gcloud artifacts repositories describe "${REPO_NAME}" --location="${REGION}" --project="${PROJECT_ID}" >/dev/null 2>&1; then
            echo "Repository exists. Attempting terraform import for google_artifact_registry_repository.todo_repo..."
            REPO_IMPORT_ID="projects/${PROJECT_ID}/locations/${REGION}/repositories/${REPO_NAME}"
            if terraform state list | grep -q "^google_artifact_registry_repository\.todo_repo$"; then
              echo "google_artifact_registry_repository.todo_repo already in state — skipping import."
            else
              echo "Importing artifact registry repo into terraform state..."
              terraform import -input=false google_artifact_registry_repository.todo_repo "${REPO_IMPORT_ID}" || {
                echo "terraform import for repo returned non-zero. Continuing; if this is 'already imported', it's ok."
              }
            fi
          else
            echo "Artifact Registry repo not found. Terraform will create it."
          fi

          # 3) Now apply Terraform (it will create missing resources and manage imported ones)
          echo "Running terraform apply..."
          terraform apply -auto-approve


    # Build Docker Images
    - task: Bash@3
      displayName: 'Build Docker Images'
      inputs:
        targetType: inline
        script: |
          docker build -t frontend:$(Build.BuildId) ./frontend
          docker build -t backend:$(Build.BuildId) ./backend

    #  Tag & Push to Artifact Registry
    - task: Bash@3
      displayName: 'Push Docker Images'
      inputs:
        targetType: inline
        script: |
          docker tag frontend:$(Build.BuildId) $REGION-docker.pkg.dev/$PROJECT_ID/$REPO_NAME/frontend:$(Build.BuildId)
          docker tag backend:$(Build.BuildId) $REGION-docker.pkg.dev/$PROJECT_ID/$REPO_NAME/backend:$(Build.BuildId)

          docker push $REGION-docker.pkg.dev/$PROJECT_ID/$REPO_NAME/frontend:$(Build.BuildId)
          docker push $REGION-docker.pkg.dev/$PROJECT_ID/$REPO_NAME/backend:$(Build.BuildId)

          echo "Successfully pushed images."

      # installing helm     
    - task: HelmInstaller@1
      inputs:
        helmVersionToInstall: '3.14.0'

    # PACKAGE HELM CHART & COLLECT DEPLOYMENT ARTIFACTS

    - task: Bash@3
      displayName: "Package Helm Chart"
      inputs:
        targetType: inline
        script: |
          echo "Packaging Helm chart..."
          mkdir -p $(Pipeline.Workspace)/deploy_artifacts

          helm package ./helm --destination $(Pipeline.Workspace)/deploy_artifacts
          cp -r terraform $(Pipeline.Workspace)/deploy_artifacts/terraform
          cp helm/values.yaml $(Pipeline.Workspace)/deploy_artifacts/values.yaml

          # Save image tags
          echo "FRONTEND_TAG=$(Build.BuildId)" >> $(Pipeline.Workspace)/deploy_artifacts/deploy_info.txt
          echo "BACKEND_TAG=$(Build.BuildId)" >> $(Pipeline.Workspace)/deploy_artifacts/deploy_info.txt
          echo "PROJECT_ID=$PROJECT_ID" >> $(Pipeline.Workspace)/deploy_artifacts/deploy_info.txt
          echo "REGION=$REGION" >> $(Pipeline.Workspace)/deploy_artifacts/deploy_info.txt
          echo "REPO_NAME=$REPO_NAME" >> $(Pipeline.Workspace)/deploy_artifacts/deploy_info.txt

          echo "Artifacts packaged."
          ls -R $(Pipeline.Workspace)/deploy_artifacts

    - publish: $(Pipeline.Workspace)/deploy_artifacts
      artifact: deploy_artifacts
      displayName: "Publish Helm + Terraform + Metadata"

        # DEBUG: Inspect CI artifact paths
    # - task: Bash@3
    #   displayName: "DEBUG: Show CI Paths"
    #   inputs:
    #     targetType: inline
    #     script: |
    #       echo "=== Pipeline.Workspace ==="
    #       echo "$(Pipeline.Workspace)"
    #       ls -R $(Pipeline.Workspace)

    #       echo "=== Agent.BuildDirectory ==="
    #       echo "$(Agent.BuildDirectory)"
    #       ls -R $(Agent.BuildDirectory)

    #       echo "=== System.DefaultWorkingDirectory ==="
    #       echo "$(System.DefaultWorkingDirectory)"
    #       ls -R $(System.DefaultWorkingDirectory)

    #       echo "=== Agent.TempDirectory ==="
    #       echo "$(Agent.TempDirectory)"
    #       ls -R $(Agent.TempDirectory)
